{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7IFJ5_Y33-8"
   },
   "source": [
    "# Lab Assignment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goqGXGZT2DKK"
   },
   "source": [
    "## Important notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3FKET1A2GRK"
   },
   "source": [
    "**Submission deadline:**\n",
    "* **Regular problems: last lab session before Friday 16.10.2020**\n",
    "* **Bonus problems: deadline for Lab Assignment 2**\n",
    "\n",
    "**Points: 11 + 3 bonus points**\n",
    "\n",
    "Please note: some of the assignments are tedious or boring if you are already a NumPy ninja. The bonus problems were designed to give you a more satisfying alternative.\n",
    "\n",
    "The assignment is in the form of a Jupyter notebook. We will be using [Google Colab](https://colab.research.google.com) to solve it. Below you will find a \"Setup\" section. Follow instructions from this paragraph to download the notebook and open it using [Google Colab](https://colab.research.google.com). \n",
    "\n",
    "Your goal is to solve problems posted below. Whenever possible, add your solutions to the notebook.\n",
    "\n",
    "Please email us about any problems with it - we will try to correct them quickly. Also, please do not hesitate to use GitHub’s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CMDOrsc2K-K"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNjavUUC7yuM"
   },
   "source": [
    "### 1. Open the notebook using Google Colab\n",
    "\n",
    "1. From Github: Click on \"View in Colaboratory\", then save to your Google Drive.\n",
    "2. Alternatively upload manually to Drive:\n",
    "  1. Download the notebook or clone https://github.com/janchorowski/ml_uwr.\n",
    "  2. Go to  [Google Colab](https://colab.research.google.com).\n",
    "  3. Go to \"UPLOAD\" tab and select a local copy of the notebook that you downloaded in point 1.\n",
    "  \n",
    "Colab Tips:\n",
    "1. Set tab width to 4 spaces under `Tools` → `Preferences`.\n",
    "  \n",
    "### 2. Open the notebook offline using Jupyter/IPython\n",
    "\n",
    "This notebook can be opened using Jupyter notebook. Simply install a scientific Python distribution on your computer (e.g. [Anaconda](https://www.anaconda.com/) or [WinPython](http://winpython.github.io/)), clone the repository https://github.com/janchorowski/ml_uwr and run `jupyter notebook`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq_ZRu87C_OC"
   },
   "source": [
    "###   3. Install required dependencies, download data and import packages\n",
    "\n",
    "Run cells below. To run a cell either click it and click a run button or press \"shift + enter\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "onui8xrw5dKi"
   },
   "outputs": [],
   "source": [
    "# Please note that this code needs only to be run in a fresh runtime.\n",
    "# However, it can be rerun afterwards too.\n",
    "!pip install -q gdown httpimport\n",
    "![ -e mnist.npz ] || gdown 'https://drive.google.com/uc?id=1QPaC3IKB_5tX6yIZgRgkpcqFrfVqPTXU' -O mnist.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "YxzWq6iO2KPm"
   },
   "outputs": [],
   "source": [
    "# Standard IPython notebook imports\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import httpimport\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.stats as sstats\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "\n",
    "# In this way we can import functions straight from github\n",
    "with httpimport.github_repo('janchorowski', 'nn_assignments', \n",
    "                            module='common', branch='nn18'):\n",
    "     from common.plotting import plot_mat\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfmjDoxi6JNA"
   },
   "source": [
    "### 4. Follow the notebook and solve problems posted below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hr81V8T8ccB"
   },
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eT1x_VG33_E"
   },
   "source": [
    "### Problem 0 [0p]\n",
    "\n",
    " \n",
    "1. To learn more about Jupyter,  read [Jupyter tutorial from Data Analysis in Biological Sciences course at Caltech](http://bebi103.caltech.edu/2015/tutorials/t0b_intro_to_jupyter_notebooks.html) (which itself can be downloaded as a Jupyter notebook). Feel free to skip the tutorial if you have some prior experience with Jupyter notebook.\n",
    "2. To learn more about basic Google Colab features, go to [Google Colab](https://colab.research.google.com) and select \"Overview of Colaboratory Features\" in \"EXAMPLES\" tab. To learn more about / set up useful keyboard shortcuts (e.g. to add a new cell without clicking \"\"+ code\"), go to \"Tools --> Keyboard shortcuts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsOC0voR33_F"
   },
   "source": [
    "### Problem 1: NumPy [2p]\n",
    "\n",
    "First, get familiar with Python at https://docs.python.org/3/tutorial/. Then, get\n",
    "to know the capabilities of NumPy, the prime numerical library of Python http://www.numpy.org/, for instance with the tutorial at https://numpy.org/doc/stable/user/quickstart.html. Finally, look into Pandas at https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html.\n",
    "\n",
    "You might also need:\n",
    "  1. another intro to NumPy,\n",
    "http://people.duke.edu/~ccc14/pcfb/numerics.html\n",
    "  2. a better interactive shell for Python\n",
    "http://ipython.org/\n",
    "  3. a plotting library for Python\n",
    "http://matplotlib.org/\n",
    "  4. nice statistical plots for matplotlib https://seaborn.pydata.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ijCVw7reGbm"
   },
   "source": [
    "\n",
    "**a) Declare variables:**\n",
    "1. $a=10$,\n",
    "2. $b=2.5\\times 10^{23}$,\n",
    "3. $c=2+3i$, where $i$ is an imaginary unit,\n",
    "4. $d=e^{i2\\pi/3}$, where $i$ is an imaginary unit, $e$ is the Euler's number (use `exp`, `pi`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "8sd7jJhd33_G"
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the declarations\n",
    "a = 10\n",
    "b = 2.5e23\n",
    "c = 2 + 3j\n",
    "d = np.exp(1j*2*np.pi/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP0hAHvN33_K"
   },
   "source": [
    "**b) Declare vectors:**\n",
    "1. $aVec=\\begin{bmatrix} 3.14 & 15 & 9 & 26 \\end{bmatrix}$,\n",
    "2. $bVec=\\begin{bmatrix} 5 & 4.8 & \\cdots & -4.8 & -5 \\end{bmatrix}$ (vector of numbers from $5$ to $-5$ decreasing by $0.2$),\n",
    "3. $cVec=\\begin{bmatrix} 10^0 & 10^{0.01} & \\cdots & 10^{0.99} & 10^1 \\end{bmatrix}$ (logarithmically spaced numbers from 1 to 10, use `logspace` and make sure, that the result has correct length!),\n",
    "4. $dVec=Hello$ ($eVec$ is a string of characters, thus a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "37RmkuW533_L"
   },
   "outputs": [],
   "source": [
    "aVec = np.array([3.14, 15, 9, 26])\n",
    "bVec = np.arange(5, -5.2, -0.2)\n",
    "cVec = np.logspace(0, 1, num = 101, endpoint = True, base = 10)\n",
    "dVec = np.array(list(\"Hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C58YJtEU33_O"
   },
   "source": [
    "**c) Declare matrices:**\n",
    "1. \\begin{equation}aMat=\\begin{bmatrix}\n",
    "                    2      & \\cdots & 2 \\\\\n",
    "                    \\vdots & \\ddots & \\vdots \\\\\n",
    "                    2      & \\cdots & 2\n",
    "          \\end{bmatrix},\\end{equation}\n",
    "    matrix $9\\times 9$ filled with 2s (use `ones` or `zeros`),\n",
    "2. \\begin{equation}bMat=\\begin{bmatrix}\n",
    "                    1      & 0      & \\cdots &        & 0      \\\\\n",
    "                    0      & \\ddots & 0      &        & 0      \\\\\n",
    "                    \\vdots & 0      & 5      & 0      & \\vdots \\\\\n",
    "                           &        & 0      & \\ddots & 0      \\\\\n",
    "                    0      &        & \\cdots & 0      & 1\n",
    "                \\end{bmatrix},\\end{equation}\n",
    "    matrix $9\\times 9$ filled with zeros, with\n",
    "    $[ 1, 2, 3, 4, 5, 4, 3, 2, 1]$ on its diagonal (use `zeros`, `diag`),\n",
    "3. \\begin{equation}cMat=\\begin{bmatrix}\n",
    "                    1      & 11     & \\cdots & 91     \\\\\n",
    "                    2      & 12     & \\cdots & 92     \\\\\n",
    "                    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                    10     & 20     & \\cdots & 100\n",
    "                \\end{bmatrix},\\end{equation}\n",
    "    matrix $10\\times 10$, columns of which form the vector $1:100$ (use `reshape`),\n",
    "4. \\begin{equation}dMat=\\begin{bmatrix}\n",
    "                    NaN & NaN & NaN & NaN \\\\\n",
    "                    NaN & NaN & NaN & NaN \\\\\n",
    "                    NaN & NaN & NaN & NaN\n",
    "                \\end{bmatrix},\\end{equation}\n",
    "    matrix $3\\times 4$ filled with `NaN`s (use... `NaN`),\n",
    "5. \\begin{equation}eMat=\\begin{bmatrix}\n",
    "                    13  & -1  & 5  \\\\\n",
    "                    -22 & 10  & -87\n",
    "                \\end{bmatrix},\\end{equation}\n",
    "6. $fMat$ of shape $3\\times 3$ filled with random natural numbers from $[-3,3]$ (use `rand` and `floor` or `ceil`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Bdd1oUeI33_P"
   },
   "outputs": [],
   "source": [
    "aMat = np.ones( (9, 9) ) * 2\n",
    "z = np.zeros( (9,9) )\n",
    "bMat = np.fill_diagonal(z, [1,2,3,4,5,4,3,2,1])\n",
    "cMat = np.arange(1,101).reshape(10,10)\n",
    "dMat = np.zeros((3,4)).fill(np.nan)\n",
    "eMat = np.array([[13,-1,5], [-22,10,-87]])\n",
    "fMat = np.random.randint(-3, 4, size = (3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqtKQi1Q33_T"
   },
   "source": [
    "**d) Declare a multiplication table**\n",
    "as a $10\\times 10$ matrix `mulMat`. Use matrix/vector multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "OWqHq-vK33_U"
   },
   "outputs": [],
   "source": [
    "mulMat = np.multiply.outer(np.arange(1,11), np.arange(1,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xIm6lT133_Y"
   },
   "source": [
    "**e) Compute element-wise using values from b).**\n",
    "\n",
    "For instance, the first element of $xVec[0]$ should be equal to\n",
    "\n",
    "\\begin{equation}\n",
    "1/(\\sqrt{2\\pi2.5^2}) e^{-cVec[0]^2 / (2\\cdot\\pi 2.5^2)}.\n",
    "\\end{equation}\n",
    "\n",
    "1. $xVec=1/(\\sqrt{2\\pi2.5^2}) e^{-cVec^2 / (2\\cdot\\pi 2.5^2)}$\n",
    "2. $yVec=\\log_{10}(1/cVec)$, using `log10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "LtH-kSU733_Z"
   },
   "outputs": [],
   "source": [
    "xVec = np.asarray([1/np.sqrt(2*np.pi*(2.5)**2)*np.exp(-c**2/(2*np.pi*(2.5)**2)) for c in cVec])\n",
    "yVec = np.asarray([np.log10(1/c) for c in cVec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBhf74F_33_d"
   },
   "source": [
    "**f) Compute with matrix/vector operations.**\n",
    "\n",
    "**NOTE:** Every multiplication (and power) in this subtask is a [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication).\n",
    "1. $xMat=[0,  1, ..., 6][0, 10, 20, ..., 60]^T$,\n",
    "2. $yMat=[0, 10, 20, ..., 60]^T[0,  1, ..., 6]$\n",
    "<br/>\n",
    "(remember, that matrix multiplication is not commutative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "UjdLR34u33_e"
   },
   "outputs": [],
   "source": [
    "a = np.array([np.arange(0,7)])\n",
    "b = np.array([np.arange(0,70,10)])\n",
    "xMat = np.matmul(a,b.T)\n",
    "yMat = np.matmul(b.T,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ3xTXgV33_i"
   },
   "source": [
    "**g) Declare `ismagic(A)` function** \n",
    "which checks if matrix $A$ is a [magic square](https://en.wikipedia.org/wiki/Magic_square) and returns a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "uCNJUx_G33_j"
   },
   "outputs": [],
   "source": [
    "def ismagic(A):\n",
    "    s = set()\n",
    "    s.update([A.diagonal().sum(), np.fliplr(A).diagonal().sum()])\n",
    "\n",
    "    for row in A:\n",
    "      s.add(row.sum())\n",
    "   \n",
    "    for column in A.T:\n",
    "      s.add(column.sum())\n",
    "\n",
    "    if len(s) > 1:\n",
    "      return False\n",
    "    return True\n",
    "    \n",
    "assert not ismagic(np.array([[1,1], [2,2]]))\n",
    "assert ismagic(np.array([[2,7,6],[9,5,1],[4,3,8]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl8_nizPeiNy"
   },
   "source": [
    "### Problem 2: Pandas and Seaborn [2p]\n",
    "\n",
    "1. Load the IRIS Data into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "FzhoE5MWerAf",
    "outputId": "903e8a79-7b1d-495f-efd6-39f6b016b4d0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width       target\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "5           5.4          3.9           1.7          0.4  Iris-setosa\n",
       "6           4.6          3.4           1.4          0.3  Iris-setosa\n",
       "7           5.0          3.4           1.5          0.2  Iris-setosa\n",
       "8           4.4          2.9           1.4          0.2  Iris-setosa\n",
       "9           4.9          3.1           1.5          0.1  Iris-setosa"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "# Use read_csv to load the data. Make sure you get 150 examples!\n",
    "iris_df = pd.read_csv(iris_url, header=None)\n",
    "\n",
    "# Set the column names to\n",
    "# 'sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'target'\n",
    "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'target']\n",
    "\n",
    "# Print the first 10 entries\n",
    "iris_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "IRV7K0gNgb2r",
    "outputId": "d5c0fb9c-4b77-432e-e445-722261a8838a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667\n",
       "std        0.828066     0.433594      1.764420     0.763161\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show numerical summary of the data, using DataFrame.describe()\n",
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHa4zZNvg0jb"
   },
   "outputs": [],
   "source": [
    "# Plot the data using seaborn's pairplot\n",
    "sns.pairplot(iris_df, hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kDC5d82oaKo"
   },
   "source": [
    "The Iris data is in a so-called 'wide' format, in which each column corresponds to a variable and each row of the DataFrame corresponds to one observation. Turn it into a 'long' format in which each row is a measurement. \n",
    "\n",
    "Specifically, change the data layout of the IRIS dataFrame so that it has 3 columns:\n",
    "- variable (one of sepal_length, sepal_width, petal_length, petal_width)\n",
    "- value\n",
    "- target\n",
    "\n",
    "If you would like to learn more, [Tidy Data](https://www.jstatsoft.org/index.php/jss/article/view/v059i10/v59i10.pdf) by [Hadley Wickham](http://hadley.nz/) provides a very nice explanation of best practices for data formating.\n",
    "\n",
    "Hint: look at reshaping functions in http://pandas.pydata.org/Pandas_Cheat_Sheet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VsSfAmwrNGu"
   },
   "outputs": [],
   "source": [
    "iris_df_long = iris_df.melt(id_vars = 'target')\n",
    "iris_df_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28A-QiuosBw8"
   },
   "source": [
    "Now create a box-plot of values that each variable takes, split by the target species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51HhmH0msNdM"
   },
   "outputs": [],
   "source": [
    "# Hint: use a `catplot`\n",
    "sns.catplot(x='target', y='value', col='variable', kind='box', data=iris_df_long)\n",
    "\n",
    "# TODO: create two more plots, using a boxenplot and a swarmplot.\n",
    "\n",
    "sns.catplot(x='target', y='value', col='variable', kind='boxen', data=iris_df_long)\n",
    "sns.catplot(x='target', y='value', col='variable', kind='swarm', data=iris_df_long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es80_WJM33_n"
   },
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "We will use the loaded Iris data describing iris flowers\n",
    "and shows relations between their length and petal width for three\n",
    "species (namely: setosa, versicolor, virginica).\n",
    "\n",
    "For this exercise we will restrict our analysis to just two variables: **petal length** and **petal width**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9dHQGslwvuc"
   },
   "outputs": [],
   "source": [
    "unknown_df = pd.DataFrame(\n",
    "    [[1.5, 0.3, 'unknown'],\n",
    "     [4.5, 1.2, 'unknown'],\n",
    "     [5.1, 1.7, 'unknown'],\n",
    "     [5.5, 2.3, 'unknown']],\n",
    "     columns=['petal_length', 'petal_width', 'target'])\n",
    "\n",
    "sns.scatterplot(x='petal_length', y='petal_width', hue='target', data=iris_df)\n",
    "sns.scatterplot(x='petal_length', y='petal_width', color='gray', marker='v',\n",
    "                label='unknown', s=70, data=unknown_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2khewKZ33_w"
   },
   "source": [
    "Based on these two features, it is easy to distinguish iris setosa from the two remaining species. Yet iris versicolor and virginica remain mixed together. \n",
    "\n",
    "Looking closely at the plot, we might estimate the species of the selected unknown irises (gray triangles). For three of them the answer seems obvious – they belong in uniformly-colored areas covered by one species only. Yet unknown iris flower in (5.1, 1.7) is troublesome – it lays on the boundary of versicolor and virginica clusters. We can assume, that its species is the one of the closest one to it, coming from the training set (and so having a label). \n",
    "\n",
    "K-Nearest Neighbors method (http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) solves the classification problem, i.e. sets class labels (species in case of irises) of a previously unseen sample by choosing the most common class among the top k neighbors of the sample in question (for instance according to the Euclidean distance). Thus, the k-Nearest Neighbors algorithm works as follows. For each unlabeled sample x:\n",
    "1. Find k nearest neighbors among the labeled samples.\n",
    "2. Set the most common label among them as label of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpyfaeVg72jo"
   },
   "source": [
    "#### Problem 3 [3p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6sG0bxf33_y"
   },
   "source": [
    "##### Implement the k-Nearest Neighbors algorithm [1p].\n",
    "\n",
    "Take advantage of matrix calculus rather than using for loops.\n",
    "\n",
    "**Tip:** What is computed by \\begin{equation} \\sqrt{(X - Y)^T (X - Y)} \\end{equation} when both X and Y are vectors?\n",
    "\n",
    "**Tip:** Try to use broadcasting (NumPy: http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) and built-ins sort, numpy.sort, numpy.argsort (sorting), scipy.stats.mode (choosing the most common element of the set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKhbqAUTz5yy"
   },
   "outputs": [],
   "source": [
    "def KNN(train_X, train_Y, test_X, ks, verbose=False):\n",
    "    \"\"\"\n",
    "    Compute predictions for various k\n",
    "    Args:\n",
    "        train_X: array of shape Ntrain x D\n",
    "        train_Y: array of shape Ntrain\n",
    "        test_X: array of shape Ntest x D\n",
    "        ks: list of integers\n",
    "    Returns:\n",
    "        preds: dict k: predictions for k\n",
    "    \"\"\"\n",
    "    # Cats data to float32\n",
    "    train_X = train_X.astype(np.float32)\n",
    "    test_X = test_X.astype(np.float32)\n",
    "\n",
    "    # Alloc space for results\n",
    "    preds = {}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computing distances... \", end='')\n",
    "    #\n",
    "    # TODO: fill in an efficient distance matrix computation\n",
    "    #    \n",
    "\n",
    "\n",
    "\n",
    "    dists = -2 * np.matmul(train_X, test_X.T) + np.sum(test_X ** 2, axis = 1) + np.sum(train_X ** 2, axis = 1).reshape(train_X.shape[0],1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Sorting... \", end='')\n",
    "    \n",
    "    # TODO: findes closest trainig points\n",
    "    # Hint: use argsort\n",
    "    closest = np.argsort(dists, axis = 0)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Computing predictions...\", end='')\n",
    "    \n",
    "    targets = train_Y[closest]\n",
    "\n",
    "    for k in ks:\n",
    "        predictions = sstats.mode(targets[:k])[0]\n",
    "        predictions = predictions.ravel()\n",
    "        preds[k] = predictions\n",
    "    if verbose:\n",
    "        print(\"Done\")\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSiSLLke2K1r"
   },
   "outputs": [],
   "source": [
    "# Now classify the 4 unknown points\n",
    "iris_x = np.array(iris_df[['petal_length', 'petal_width']])\n",
    "iris_y = np.array(iris_df['target'])\n",
    "\n",
    "unknown_x = np.array(unknown_df[['petal_length', 'petal_width']])\n",
    "\n",
    "KNN(iris_x, iris_y, unknown_x, [1, 3, 5, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZ3Ef8Np7ZU8"
   },
   "source": [
    "##### Plot the Decision boundary [1p]\n",
    "\n",
    "\n",
    "Use meshgrid to generate the points in the space spanned by data.\n",
    "Then map the classes to numbers 0, 1, 2 and make a contour plot with the\n",
    "decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3aeNd0E3r8w"
   },
   "outputs": [],
   "source": [
    "iris_x = np.array(iris_df[['petal_length', 'petal_width']])\n",
    "iris_y = np.array(iris_df['target'])\n",
    "\n",
    "\n",
    "mesh_x, mesh_y = np.meshgrid(np.linspace(0,8,200), np.linspace(0,3,200))\n",
    "\n",
    "\n",
    "#use np.unique with suitable options to map the class names to numbers\n",
    "target_names, iris_y_ids = np.unique(iris_y ,return_inverse = True)\n",
    "\n",
    "mesh_data = np.hstack([mesh_x.reshape(-1, 1), mesh_y.reshape(-1, 1)])\n",
    "\n",
    "preds = KNN(iris_x, iris_y_ids, mesh_data, [1, 3, 5, 7])\n",
    "\n",
    "for k, preds_k in preds.items():\n",
    "    plt.figure()\n",
    "    plt.title(f\"Decision boundary for k={k}\")\n",
    "    plt.contourf(mesh_x, mesh_y, preds_k.reshape(200, 200))\n",
    "    plt.scatter(iris_x[:,0], iris_x[:,1], c=iris_y_ids, edgecolors='white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iZtHs5A33_z"
   },
   "source": [
    "##### Estimate performance for various ks [1p]\n",
    "Consider the following experiment (bootsrap error estimation):\n",
    "1. We sample **with replacement** a dataset equal in size to the amount of data we have (i.e. for Iris, we pick 150 examples). This forms a trainign set. Since we have sampled with replacement, some samples were picked multiple times, and some were left out. Form a test set from the remaining ones.\n",
    "2. Based on the training set, we use the k-NN algorithm to predict the labels on the test set.\n",
    "3. We then check the number of errors and write it down.\n",
    "\n",
    "Do this 500 times for k ∈ {1, 3, 5, ..., 19}. Plot a function of the average number of errors as the function of k. It should be similar to one of the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24FeneG8A4Hi"
   },
   "outputs": [],
   "source": [
    "#TODO: write a function to compute error rates\n",
    "def err_rates(preds, test_Y):\n",
    "    ret = {}\n",
    "    for k, preds_k in  preds.items():\n",
    "        # TODO: fill in error count computation\n",
    "        ret[k] = np.sum(preds_k!=test_Y) / len(test_Y)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7U4ejQYy1BRT"
   },
   "outputs": [],
   "source": [
    "iris_x = np.array(iris_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "iris_y = np.array(iris_df['target'])\n",
    "\n",
    "ks = range(1, 30, 2)\n",
    "results = []\n",
    "\n",
    "for _rep in tqdm(range(1000)):\n",
    "    #TODO\n",
    "    # Use np.random.randint to get training indices\n",
    "    # The find all unselected indices to form a test set \n",
    "    train_idx =  np.random.randint(0,149,150)\n",
    "    #train_idx =  np.unique(train_idx)\n",
    "    test_idx = np.setdiff1d(np.arange(0,149), train_idx)\n",
    "    #TODO: apply your kNN classifier to data subset\n",
    "    \n",
    "    preds = KNN(iris_x[train_idx], iris_y[train_idx], iris_x[test_idx], ks)\n",
    "    errs = err_rates(preds, iris_y[test_idx])\n",
    "    \n",
    "    for k, errs_k in errs.items():\n",
    "        results.append({'K':k, 'err_rate': errs_k})\n",
    "\n",
    "# results_df will be a data_frame in long format\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.regplot(x='K', y='err_rate', data=results_df, x_estimator=np.mean, order=3)\n",
    "plt.figure()\n",
    "sns.regplot(x='K', y='err_rate', data=results_df, x_estimator=np.mean, lowess=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozuWZxLXXxqa"
   },
   "source": [
    "#### Problem 4 [2p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7C17WB7aEIo"
   },
   "source": [
    "Implement the k-NN algorithm with a leave-one-out error estimation (Note: it can be done very efficiently).\n",
    "\n",
    "Then perform two experiments:\n",
    "1. plot the leave-one-out error rate as a function of $K$\n",
    "2. choose K=15, plot the leave-one-out error rate as a function of training dataset size. For best results, repeath the experiment 100 times with different permutations of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhx4UjY4X3Sz"
   },
   "outputs": [],
   "source": [
    "iris_x = np.array(iris_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "iris_y = np.array(iris_df['target'])\n",
    "\n",
    "ks = range(1, 30, 2)\n",
    "results = []\n",
    "\n",
    "for idx in range(149):\n",
    "    #TODO\n",
    "    # Use np.random.randint to get training indices\n",
    "    # The find all unselected indices to form a test set \n",
    "    test_idx =  np.array([idx])\n",
    "    train_idx = np.setdiff1d(np.arange(0,149), train_idx)\n",
    "    #TODO: apply your kNN classifier to data subset\n",
    "      \n",
    "    preds = KNN(iris_x[train_idx], iris_y[train_idx], iris_x[test_idx], ks)\n",
    "    errs = err_rates(preds, iris_y[test_idx])\n",
    "      \n",
    "    for k, errs_k in errs.items():\n",
    "        results.append({'K':k, 'err_rate': errs_k})\n",
    "\n",
    "# results_df will be a data_frame in long format\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.regplot(x='K', y='err_rate', data=results_df, order=2, x_estimator=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhRGLfUmX4Op"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for sz in range(1, 149, 2):\n",
    "  for j in range(100):\n",
    "    train_idx = np.random.choice(149, sz, replace=False)\n",
    "    test_idx = np.setdiff1d(np.arange(0,149), train_idx)\n",
    "    \n",
    "    preds = KNN(iris_x[train_idx], iris_y[train_idx], iris_x[test_idx], [15])\n",
    "    errs = err_rates(preds, iris_y[test_idx])\n",
    "    \n",
    "    for k, errs_k in errs.items():\n",
    "        results.append({'data_size':sz, 'err_rate': errs_k})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.lineplot(x='data_size', y='err_rate', data=results_df, estimator=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYGjasc_Y4Ac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A86GnZpa33_0"
   },
   "source": [
    "#### Problem 5 [2p] \n",
    "\n",
    "Apply the K-Nearest Neighbors (K-NN) algorithm to the MNIST dataset. \n",
    "\n",
    "The MNIST (http://yann.lecun.com/exdb/mnist/) dataset consists of normalized (centered and stretched) scans of hand-written digits. Specifically, each element of the dataset is a 28 × 28 grayscale image, thus having 764 8-bit pixels. \n",
    "\n",
    "1. Display a few objects from each of the classes, paying attention to aesthetics and clarity of your presentation. **Note:** You already downloaded the dataset in \"Setup\" section. Please use the code below to get started.\n",
    "\n",
    "2. **[2p]** Apply a k-NN classifier to the MNIST dataset. First, divide the training set into two parts, which we will call training and validation. On MNIST use the first 50000 samples for training and the last 10000 for validation. Then find the optimal number of neighbors by assessing the accuracy on the validation set. You do not need to repeat this experiment multiple times. Finally, compute the accuracy on the test set obtained with the best previously chosen number of neighbors. On MNIST you should get about 3% errors. Pick a few mislabeled samples from the test dataset and plot them along with the correct ones. **Note:**\n",
    "  * MNIST is much larger than the Iris dataset. A good implementation may need a few minutes depending on your runtime type. Please optimize your algorithm:\n",
    "  * Compute the distances only once, then test for different values of k.\n",
    "  * Use vectorized expressions to compute the distance. It is possible to compute all distances between the training and testing points in one expression. Hint: think about the vectorized expression \\begin{equation}(X - Y)^T (X - Y)\\end{equation}\n",
    "  * You can use single precision numbers in computation.\n",
    "  * If your code is taking a long time to execute, please save its results before the lab session.\n",
    "\n",
    "**Note:** in NumPy, matrices have its own data type (dtype), which is retained during\n",
    "calculations. Please pay attention to it. I particular, do not subtract values of data types not\n",
    "having the sign bit, do not divide integers, etc. Results of such operations will not be\n",
    "automatically casted to types having the required precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AvAeiDN33_1"
   },
   "outputs": [],
   "source": [
    "with np.load('mnist.npz') as data:\n",
    "    mnist_full_train_data_uint8 = data['train_data']\n",
    "    mnist_full_train_labels_int64 = data['train_labels']\n",
    "    mnist_test_data_uint8 = data['test_data']\n",
    "    mnist_test_labels_int64 = data['test_labels']\n",
    "        \n",
    "# Split train data into train and validation sets\n",
    "mnist_train_data_uint8 = mnist_full_train_data_uint8[:50000]\n",
    "mnist_train_labels_int64 = mnist_full_train_labels_int64[:50000]\n",
    "mnist_valid_data_uint8 = mnist_full_train_data_uint8[50000:]\n",
    "mnist_valid_labels_int64 = mnist_full_train_labels_int64[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aenFc3L633_4"
   },
   "outputs": [],
   "source": [
    "plot_mat(mnist_train_data_uint8[:20, None], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZK46uiW4-Yox"
   },
   "outputs": [],
   "source": [
    "# MNIST is large.\n",
    "# Implement a batched KNN classifier, which processes the test data in small batches\n",
    "# and returns the error rates\n",
    "\n",
    "# The code should not run for more than a couple of minutes on the Colab runtime, \n",
    "# If it is slower, optimize the distance computation in KNN\n",
    "\n",
    "def batched_KNN(train_X, train_Y, test_X, ks, verbose=False, batch_size=200):\n",
    "    all_preds = {k: [] for k in ks}\n",
    "    for i in tqdm(range(0, test_X.shape[0], batch_size)):\n",
    "        batch_X = test_X[i:i + batch_size]\n",
    "        \n",
    "        # TODO: run KNN on the batch and save the predictions\n",
    "        for k in all_preds.keys():\n",
    "          preds = KNN(train_X, train_Y, batch_X, ks)\n",
    "\n",
    "        # TODO: combine predictions from batches\n",
    "          all_preds[k] = np.concatenate([np.array(all_preds[k]), np.array(preds[k])])\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZB49qMMCP9i"
   },
   "outputs": [],
   "source": [
    "# Now find the best k on the validation set\n",
    "ks = [1, 3, 5, 7, 9]\n",
    "mnist_validation_preds = batched_KNN(\n",
    "    mnist_train_data_uint8.astype('float32').reshape(-1, 28*28), mnist_train_labels_int64,\n",
    "    mnist_valid_data_uint8.astype('float32').reshape(-1, 28*28),\n",
    "    ks)\n",
    "\n",
    "mnist_validation_errs = err_rates(mnist_validation_preds, mnist_valid_labels_int64)\n",
    "plt.plot(ks, [mnist_validation_errs[k] for k in ks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5j4eGVN34AR"
   },
   "outputs": [],
   "source": [
    "# Now use the best k to compute the test error\n",
    "\n",
    "best_K = 3\n",
    "\n",
    "mnist_test_preds = batched_KNN(\n",
    "    mnist_full_train_data_uint8.astype('float32').reshape(-1, 28*28), \n",
    "    mnist_full_train_labels_int64,\n",
    "    mnist_test_data_uint8.astype('float32').reshape(-1, 28*28), \n",
    "    [best_K])\n",
    "\n",
    "mnist_test_errs = err_rates(mnist_test_preds, mnist_test_labels_int64)\n",
    "print(f\"\\n\\nWhen k={best_K} the test error rate is {mnist_test_errs[best_K] * 100.0:.1f}%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qq8XLOt34AX"
   },
   "source": [
    "### Locality sensitive hashing\n",
    "\n",
    "Problem 4 was about speeding up the inference using loops implicitly present in matrix multiplication instead of explicit loops in Python. In this problem, we will explore a strategy to truly reduce the total number of computations required to find nearest neighbors without sacrificing too much accuracy.\n",
    "\n",
    "To speed up nearest neighbor search we will employ *Locality Sensitive Hashing (LSH)* functions. For a given distance metric, the locality sensitive hash should put items that are similar into the same bucket. Notice that this is essentially a design choice opposite to traditional cryptographic hash functions that should amplify the difference of similar inputs (typically we want that small perturbations of data result in large changes to the hash value).\n",
    "\n",
    "One of the simplest implementations of LSH approximates the cosine distance. Let $x\\in \\mathbb{R}^N$ and $y\\in \\mathbb{R}^N$ be two vectors. Their cosine distance is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    d_\\text{cos}(x,y) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|} = \\cos\\left(\\theta(x,y)\\right),\n",
    "\\end{equation}\n",
    "where $\\theta(x,y)$ is the unsigned angle between $x$ and $y$.\n",
    "\n",
    "We will construct a family $H$ of hash functions that are an LSH for angle distances (an approximation to cosine distance). Assume $p\\in \\mathbb{R}^N$ is a random vector (components are sampled from the normal distribution) of length 1. Then define the hash function $h(x) = \\text{sgn}(x\\cdot p)$, where $\\text{sgn()}$ is the sign function. It can be proven that:\n",
    "\n",
    "\\begin{equation}\n",
    "    p_{h\\in H}[h(x)=h(y)] = 1 - \\frac{\\theta(x,y)}{\\pi}.\n",
    "\\end{equation}\n",
    "\n",
    "The equation means that the probability of a hash collision grows as the the angle between two vectors gets smaller. Therefore, vectors that are close according to the cosine distance will be put with high probability into the same bin (we use the fact that for small $\\theta$ we can approximate $\\cos(\\theta) = 1 - \\theta/\\pi$.\n",
    "\n",
    "We will say that a family of randomly chosen hash functions $H$ is $(d_1, d_2, p_1, p_2)$-sensitive with respect to a distance metric $d$ if for any $x$ and $y$:\n",
    "1. If $d(x,y) \\leq d_1$ then $p_{h\\in H}[h(x)=h(y)] \\geq p_1$.\n",
    "2. If $d(x,y) \\geq d_2$ then $p_{h\\in H}[h(x)=h(y)] \\leq p_2$.\n",
    "\n",
    "For example, our family of randomly chosen hyperplanes is $(d_1, d_2, (1-d_1)/\\pi, (1-d_2)/\\pi)$-sensitive.\n",
    "\n",
    "Ideally, vectors should be placed into the same bin with a high probability if their distance is smaller than a threshold, and with a low probability if their distance is larger that the threshold. By combining hashing functions we can get closer to this ideal sensitivity.\n",
    "\n",
    "Given a family of hash functions $H$ with sensitivity $(d_1, d_2, p_1, p_2)$ we can construct a new family $H'$ by combining $r$ functions from $H$:\n",
    "1. AND: let $h=[h_1, h_2, \\ldots, h_r] \\in H'$ and $h(x)=h(y)$ if and only if $\\forall_i h_i(x)=h_i(y)$. Then $H'$ is $(d_1, d_2, (p_1)^r, (p_2)^r)$-sensitive.\n",
    "2. OR: let $h=[h_1, h_2, \\ldots, h_r] \\in H'$ and $h(x)=h(y)$ if and only if $\\exists_i h_i(x)=h_i(y)$. Then $H'$ is $(d_1, d_2, 1-(1-p_1)^r, 1-(1-p_2)^r)$-sensitive.\n",
    "\n",
    "AND makes all probabilities shrink, but properly choosing $r$ we can make the lower probability approach 0 while the higher does not. Conversely, OR makes all probabilities grow, we can make the upper probability approach 1 while the lower does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OutHmcfp8zLo"
   },
   "source": [
    "#### Problem 6 [1-3p bonus] \n",
    "\n",
    "1. **[1bp for exercises list]** **Note:** you can show sketches of proofs for this assignment.\n",
    "    1. Show that angle between vectors is a metric (https://en.wikipedia.org/wiki/Metric_(mathematics)).\n",
    "    \n",
    "    2. Show that $p_{h\\in H}[h(x)=h(y)] = 1 - \\frac{\\theta(x,y)}{\\pi}$ for $h$ computed using a randomly chosen hyperplane.\n",
    "\n",
    "    3. Show the properties of either AND or OR boosting of LSH.\n",
    "\n",
    "  Please show the solution to this problem dirung the Session for Homework 1, the bonus point will also be added to the points from Homework 1.\n",
    "\n",
    "3. **[1-3bp]** Reimplement k-Nearest Neighbors for MNIST classification using the cosine distance instead of the Euclidean distance. Choose a sensible value of $k$. Use Locality Sensitive Hashing to achieve an error rate no greater than $150\\%$ of the original error rate with at least a $90\\%$ speedup (i.e., by considering on average at most 5000 training samples per query image). For a few settings plot the speedup-vs-accuracy relation.\n",
    "\n",
    "  **Note:** points will be awarded based on ingenuity of your solution. Feel free to explore your own ideas!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfDUaFvAXVO2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
