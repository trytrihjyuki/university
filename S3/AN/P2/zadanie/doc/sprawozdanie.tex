\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{polski}

\usepackage{movie15}
\usepackage{amssymb, amsmath, amsfonts, amsthm, cite, mathtools, enumerate, rotating, hyperref, enumitem, graphicx, subfig,algorithmic,algorithm}
\newcommand \eq[1]{\begin{equation} \begin{split}  #1 \end{split} \end{equation}}
\graphicspath{ {../../wykresy/} }

\makeatletter
\newcommand\tab[1][1cm]{\hspace*{#1}}
\def\@seccntformat#1{%
  \expandafter\ifx\csname c@#1\endcsname\c@section\else
  \csname the#1\endcsname\quad
  \fi}
\makeatother

\title{Sprawozdanie do zadania \textbf{P2.03}}
\date{13.12.2020}
\author{Maurycy Borkowski}
\begin{document}
\maketitle
\section{Przedstawienie problemu}
W zadaniu mamy przedstawić algorytm konstrukcji wielomianu $w$ możliwie najniższego stopnia, który dla $\varepsilon > 0$ spełnia nierówność:
\begin{equation}
\int_0^1 x[f(x) - w(x)]^2 dx < \varepsilon
\end{equation}
Szukamy więc wielomianu optymalnego $w^*$ w sensie aproksymacji średnio kwadratowej dla funkcji wagowej $p(x) = x$. Najniższego stopnia żeby spełniony był warunek (1).\\\\
Mamy zdefiniowany iloczyn skalarny:
\begin{equation}
\langle f,g\rangle = \int_0^1 p(x)f(x)g(x) dx = \int_0^1 xf(x)g(x) dx
\end{equation}
Szukany $w^*$ ma spełniać:
\begin{equation}
\Vert f - w^*\Vert^2 = \langle f-w^*,f-w^*\rangle = \int_0^1 x[f(x) - w^*(x)]^2 dx < \varepsilon
\end{equation}
\section{Szukanie wielomianu optymalengo}
Korzystamy z faktu (\textbf{M7.5}):
\begin{equation}
w^*_n \text{ jest optymalny} \iff \langle f-w_n^*,w_i\rangle = 0 \iff \langle f,w_i\rangle = \langle w_n^*,w_i\rangle
\end{equation}
$w^*_n \in \Pi_n$ więc możemy go rozpisać jednoznacznie w bazie: $\{1,x,x^2,\ldots,x^n\}$:
\begin{equation}
w^*_n = \sum_{i=0}^n \alpha_i \cdot x^i
\end{equation}
Z (4) i (5):
\begin{equation}
\langle f,w_i\rangle = \langle w_n^*,w_i\rangle = \langle\sum_{i=0}^n \alpha_i \cdot x^i,w_i\rangle
\end{equation}
teraz z liniowości iloczynu skalarnego:
\begin{equation}
\langle f,w_i\rangle = \langle w_n^*,w_i\rangle = \sum_{i=0}^n \alpha_i \cdot \langle x^i,w_i\rangle
\end{equation}
Z powyższych rozważań wynika, że aby wyznaczyć wielomian optymalny $w^*_n$ wystarczy, że wyznaczymy $\alpha_0, \ldots, \alpha_n$ spełniające (7), rozpiszmy to w bardziej intuicyjnej postaci macierzowej:
\begin{equation}
\begin{bmatrix}
\langle 1,1 \rangle & \langle 1,x \rangle & \ldots & \langle 1,x^n \rangle\\
\langle x,1 \rangle & \langle x,x \rangle & \ldots & \langle x,x^n \rangle\\
\vdots & \vdots & & \vdots\\
\langle x^n,1 \rangle & \langle x^n,x \rangle & \ldots & \langle x^n,x^n \rangle\\
\end{bmatrix} \cdot
\begin{bmatrix}
\alpha_0 \\
\alpha_1 \\
\vdots\\
\alpha_n \\
\end{bmatrix} =
\begin{bmatrix}
\langle f,1 \rangle \\
\langle f,x \rangle \\
\vdots \\
\langle f,x^n \rangle \\
\end{bmatrix}
\end{equation}
Pozostaje jeszcze kwestia uzupełnienia macierzy iloczynów skalarnych $n \times n$.
Dobór bazy $\Pi_n$ był nieprzypadkowy, $i,j-ty$ wyraz tej macierzy możemy obliczać w czasie stałym:
\begin{equation}
\langle x^i,x^j \rangle = \int_0^1 x\cdot x^i \cdot x^j dx = \int_0^1 x^{i+j+1} dx = \frac{x^{i+j+2}}{i+j+2} \Big|_0^1 = \frac{1}{i+j+2}
\end{equation}
Podsumowując, aby znaleźć wielomian optymalny $w^*_n$ wystarczy rozwiązać układ równań zadany (8). Będziemy go rozwiązywać za pomocą macierzy odwrotnej znalezionej za pomocą gotowych funkcji wbudowanych (nie poznawaliśmy jeszcze metod do rozwiązywania tego typu problemów).\\\\
Musimy rozwiązać jeszcze dwa podproblemy:
\begin{itemize}
    \item Liczenie nietrywialnych iloczynów skalarnych, $\langle f,x^i\rangle$ z (8)
    \item Wybór optymalnego $n$
\end{itemize}
\subsection*{Obliczanie $\langle f, g\rangle$}
Aby móc obliczać wyrazy wektora (prawa strona) z (8) typu $\langle f,x^i \rangle$ oraz żeby liczyć błąd (1) musimy być w stanie liczyć wartości całek.\\\\
Do tego skorzystamy z Metody Romberga.\\\\
Polega ona na wyznaczeniu tablicy przybliżeń wartości całek, $R_{i,j}$ w następujący sposób:\\\\\\\\\\\\\\
$R_{0,i}$ jest wzorem z metody trapezów (z podziałem odcinka na $2^i$ części), po obliczeniu pierwszej kolumny tzw. tablicy Romberga, kolejne kolumny obliczane są rekurencyjnie, otrzymując coraz lepsze przybliżenie funkcji:\\\\
\begin{equation}
\begin{cases}
\displaystyle R_{0,i} &: R_{2^i}=h_i\cdot \displaystyle \sum_{k=0}^{2^i-1}\left(\frac{f(x_k)+f(x_{k+1})}{2}\right) \\
\displaystyle R_{m,i} &: \displaystyle \frac{4^m\cdot R_{m-1,i+1}-R_{m-1,i}}{4^m-1}
\end{cases}
\end{equation}
\\Można wykazać że:
$$
R_{m,i} = I - c^*_mh_i^{2m+2}- \ldots \quad (i\geq0 ; m\geq 1)
$$
Widzimy zatem, że błąd metody Romberga dla wyrazu $R_{n,m}$ jest rzędu $\mathcal{O}(h_n^{2m+2})$ gdzie $h = \frac{1}{2^n}(b-a)$.\\\\
Algorytm obliczający przybliżenia wartości całki za pomocą Metody Romberga:
\begin{algorithm}
\begin{algorithmic}
\STATE \textbf{input:} $a,b,M,f$
\STATE $h \leftarrow a-b$
\STATE $R_{0,0} \leftarrow \frac{1}{2}(b-a)|f(a)-f(b)|$
\FOR{$n \gets 1$ to $M$}
    \STATE $h \leftarrow \frac{h}{2}$
    \STATE $R_{n,0} \leftarrow \frac{1}{2} R_{n-1,0} + h\sum_{i=1}{2^{n-1}}f(a+(2i-1)h)$
    \FOR{$m \gets 1$ to $n$}
        \STATE $R_{n,m} \leftarrow R_{n,m-1} + (R_{n,m-1} - R_{n-1,m-1}) / (4^m-1)$
        \PRINT $n,m,R_{n,m}$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
\\
Jest to algorytm pokazujący ideę tej metody, we właściwej implementacji usprawnimy go trochę. Zauważmy, że nie musimy trzymać w pamięci całej dwuwymiarowe tablicy $R[]$ wystarczy ostatni wiersz, obliczenia możemy przerwać przy zadowalającej nas dokładności (nie musimy liczyć aż do $R_{M,0}$).
\subsection*{Wybór $n$}
Dla danego $n$ możemy policzyć wielomian optymalny, $w_n^*$ z rozwiązania (8) następnie stosując Metodę Romberga policzyć błąd (1). To już jesteśmy w stanie zrobić z powyższych wniosków i pomysłów. Teraz pytanie jak dobrać optymalne $n$ to znaczy, najmniejsze spełniające (1)?\\
Oczywiście zwiększanie $n$ (stopnia wielomianu $w_n^*$) nie zmniejsza dokładność tj. może zmniejszać $\Vert f - w_n^*\Vert$.\\\\\\
Zauważmy jeszcze, że jeżeli $w_n^*$ spełnia (3) to dla dowolnego $k > n$ $w_k^*$ również spełnia warunek (3). Zakres poszukiwań jest więc \textit{uporządkowany} możemy wyszukać binarnie optymalnego $n$ ($\mathcal{O}(\log_2{n})$). W przypadku naszego problemu zakres $n$ nie jest duży, będziemy się po prostu iterować od $n = 0$ aż, $w_n^*$ będzie spełniał (3), rozważmy więc algorytm:
\begin{algorithm}
\begin{algorithmic}
\STATE \textbf{input:} $\varepsilon, f$
\STATE $n \leftarrow 0$
\STATE {$error \leftarrow get\_error(n,f)$}
\WHILE {$error \geq \varepsilon$}
    \STATE {$error \leftarrow get\_error(n,f)$}
    \STATE {$n \leftarrow n+1$}
\ENDWHILE
\RETURN $r$
\end{algorithmic}
\end{algorithm}
\\Obliczanie błędu (funkcja $get\_error(n,f)$) to po prostu obliczenie $\Vert f - w_n^*\Vert^2$ ($\mathcal{O}({n})$).
\section{Implementacja}
Cały algorytm będziemy implementowali w języku Julia, używając podwójnej
precyzji, Float64.\\
Do rozwiązywania (8) skorzystamy z
$$
A \cdot B = C \iff B = A^{-1}\cdot C
$$
Dla odwracalnej $A$.\\\\
Macierz odwrotną macierzy iloczynów skalarnych $n \times n$ policzymy z funkcji wbudowanej języka Julia: 
\href{https://docs.julialang.org/en/v1/base/math/#Base.inv-Tuple{Number}}{\textbf{inv()}}
\section{Wyniki i interpretacja}
Nasz pomysł sprawdziliśmy na następujących funkcjach: $\sin{x}$, $e^x$, $x^{-1}$, $\sqrt{x}$, $\sin{\frac{1}{x}}$ i funkcjach wielomianowych.\\\\
Zgodnie z intuicją wielomiany nie stanowiły problemu (zwłaszcza niskiego stopnia), bardzo szybko udawało się je przybliżyć wielomianem optymalnym.\\\\
Funkcje trygonometryczne w pobliżu $0$ (badamy na przedziale $[0,1]$) zachowują się jak funkcja liniowa, stąd je też szybko udało się przybliżyć.\\\\
Największym wyzwaniem okazały się funkcje nieciągłe lub nieokreślone na zadanym przedziale. Oczywiście $x^{-1}$ nie jest określony dla $x = 0$ dlatego patologiczne przypadki zastępowaliśmy podobnymi $(x+\varepsilon)^{-1}$.\\\\
Dla niektórych funkcji, algorytm nie był w stanie znaleźć dostatecznie dobrego $w_n^*$. W kolejnych iteracjach wynik wcale się nie poprawiał, wynika to z ograniczeń precyzji obliczeń. Mały błąd dla wielomianów dużego stopnia, znacząco zmieniał wynik (\href{https://en.wikipedia.org/wiki/Overfitting}{\textbf{overfitting}})\\\\
Działanie algorytmu najlepiej, zobaczyć na animacji, której tu niestety nie można umieścić, ale są one załączone w rozwiązaniu.\\\\
Dla każdej z funkcji określałem eksperymentalnie najmniejszy błąd, dla którego byliśmy w stanie znaleźć wielomian spełniający (1), oto wyniki działania dla testowanych funkcji:\\\\
\begin{tabular}{ |p{5cm}|p{3cm}|p{3cm}| }
 \hline
 \multicolumn{3}{|c|}{Wyniki} \\
 \hline
 funkcja   & maksymalny błąd $\varepsilon$ & liczba iteracji\\
 \hline
 $\sin{x}$&$10^{-9}$&5\\
 $tg{x}$&$10^{-9}$&8\\
 $e^x$&$10^{-9}$&5\\
 $x^{-1}$&$1$&8\\
 $\sqrt{x}$&$10^{-6}$&5\\
 $ 0.5-2\cdot x + 0.01\cdot x^2 + 100\cdot x^3$&$10^{-9}$&4\\
 $ 3 + 3.5\cdot x + x^2 - 1000\cdot x^4$&$10^{-9}$&5\\
 $ \sin{\frac{1}{x}}$&$2\cdot 10^{-2}$&5\\
 \hline
\end{tabular}
\section{Podsumowanie}
Przybliżanie funkcji za pomocą wielomianu z iloczynem skalarnym (2) najlepiej wychodzi dla funkcji, które gwałtownie się nie zmieniają, nie wymaga to wielomianów wysokiego stopnia. W przeciwnym przypadku, ograniczona precyzja wynikająca z obliczeń na komputerze nie pozwala nam dobrze posługiwać się wielomianami wysokich stopnii.\\
Wyniki gwałtownie się zmieniają z powodu małych błędów, dobrze to ilustruje animacja
\textbf{sininv.gif} gdzie po ósmej iteracji, zwiększanie $n$ wcale nie poprawia wyniku, dokładność $w_n^*$ względem funkcji $\sin{\frac{1}{x}}$ drastycznie spada.
\end{document}

