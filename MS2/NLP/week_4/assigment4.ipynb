{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef34449",
   "metadata": {},
   "source": [
    "### Assigment 4\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* get at least 4 points by 17.05.2023\n",
    "* remaining points: last lab session before or on 26.05.2023\n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files should be on Drive: <https://drive.google.com/drive/folders/1HaMbhzaBxxNa_z_QJXSDCbv5VddmhVVZ?usp=sharing> \n",
    "\n",
    "## Task 1 (5 points)\n",
    "\n",
    "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left. The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
    "\n",
    "<pre>\n",
    "word1 x1_1 x1_2 ... x1_N \n",
    "word2 x2_1 x2_2 ... x2_N\n",
    "...\n",
    "wordK xK_1 xK_2 ... xk_N\n",
    "</pre>\n",
    "\n",
    "Use the loss from Slide 25 in Lecture NLP.01, compute the gradient manually. You can use some gradient clipping, or regularisation. \n",
    "\n",
    "**Remark**: the data is specially prepared to make the learning process easier. \n",
    "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cac4d8-530b-4fbb-a550-13a8100942a4",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb95d621-3187-4a3b-8a33-3877fd6c249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import gzip\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36d42a-5eb1-440e-b8ca-b88f888c0d7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ex. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35c78ae3-953d-4567-8da1-7fbc3a325f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowoDwaWektor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.embedding_size = 120\n",
    "        self.LR = 0.005\n",
    "        self.num_epochs = 4\n",
    "        self.neg_samples = 15\n",
    "        self.log_every = 2000\n",
    "    \n",
    "    def load_data(self, data_path):\n",
    "        with open(data_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.data = []\n",
    "        for line in lines:\n",
    "            if random.random() < 0.01:\n",
    "                self.data.append(line.strip().split())\n",
    "        self.full_vocab = []\n",
    "        self.full_vocab_unique = set()\n",
    "        for p1,p2 in self.data:\n",
    "            self.full_vocab.append(p1)\n",
    "            if '_' in p2:\n",
    "                self.full_vocab.append(p2.split('_')[1])\n",
    "            else:\n",
    "                self.full_vocab.append(p2)\n",
    "        self.full_vocab_unique = list(set(self.full_vocab))\n",
    "        print(f'[+] Loaded {len(self.data)} lines of words!')\n",
    "    \n",
    "    def loss(self, context_vector, target_vector, negative_vectors):\n",
    "        pos_loss = -np.log(soft_max((target_vector@context_vector)))\n",
    "        neg_loss = -np.sum(np.log(soft_max((negative_vectors@context_vector))))\n",
    "        return pos_loss + neg_loss\n",
    "    \n",
    "    def get_negative_sample(self, target):\n",
    "        sampled_indices = []\n",
    "        while len(sampled_indices) < self.neg_samples:\n",
    "            neg_words = random.choices(self.full_vocab_unique, k=self.embedding_size, weights=self.word_prob)\n",
    "            if target not in neg_words:\n",
    "                sampled_indices.extend([self.word_to_index[word] for word in neg_words])\n",
    "        return np.array(sampled_indices)\n",
    "    \n",
    "    def train(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        vocab_size = len(self.full_vocab_unique)\n",
    "        for i, word in enumerate(self.full_vocab_unique):\n",
    "            self.word_to_index[word] = i\n",
    "            self.index_to_word[i] = word\n",
    "\n",
    "        # self.word_vectors = np.random.rand(vocab_size, self.embedding_size) # random initialization\n",
    "        self.word_vectors = np.random.uniform(-1, 1, (vocab_size, self.embedding_size))\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0.0\n",
    "            itek = 0\n",
    "            loss_tab = []\n",
    "            for target_word, context_word in self.data:\n",
    "                if '_' in context_word:\n",
    "                    context_word = context_word.split('_')[1]\n",
    "                target_index = self.word_to_index[target_word]\n",
    "                context_index = self.word_to_index[context_word]\n",
    "                negative_indices = self.get_negative_sample(target_word)\n",
    "\n",
    "                target_vector = self.word_vectors[target_index]\n",
    "                context_vector = self.word_vectors[context_index]\n",
    "                negative_vectors = self.word_vectors[negative_indices]\n",
    "\n",
    "                loss = self.loss(context_vector, target_vector, negative_vectors)\n",
    "                loss_tab.append(loss)\n",
    "                total_loss += loss\n",
    "\n",
    "                self.word_vectors[target_index] -= self.LR * ((soft_max(context_vector@target_vector) - 1) * target_vector + (soft_max(negative_vectors@context_vector)[:,None] * negative_vectors).sum(0))\n",
    "                self.word_vectors[context_index] -= self.LR * (soft_max(context_vector@target_vector) - 1) * context_vector\n",
    "                self.word_vectors[negative_indices] -= self.LR * (soft_max(negative_vectors@context_vector)[:,None] * negative_vectors)\n",
    "                if itek % self.log_every == 0:\n",
    "                    print(f'avg_loss: {sum(loss_tab)/self.log_every}, total_loss: {total_loss}')\n",
    "                    loss_tab = []\n",
    "                itek += 1\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {total_loss}\")\n",
    "\n",
    "    def save_to_file(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            for index, word in self.index_to_word.items():\n",
    "                vector = self.word_vectors[index]\n",
    "                vector_str = ' '.join([str(val) for val in vector])\n",
    "                f.write(f\"{word} {vector_str}\\n\")\n",
    "    \n",
    "    def filter_data(self, min_occurance=10):\n",
    "        cnt = dict(Counter([word for word in self.full_vocab]))\n",
    "        filtered_data = list(filter(lambda p: cnt[p[0]] >= min_occurance, self.data))\n",
    "        self.cnt = cnt\n",
    "        print(f'Old data size: {len(self.data)}\\nNew data size: {len(filtered_data)}\\nRemoving{100 * (len(self.data)-len(filtered_data))/(len(self.data)):.2f}%')\n",
    "        old_size = len(self.data)\n",
    "        self.data = filtered_data\n",
    "        self.word_prob = [(self.cnt[w]**0.75)/old_size for w in self.full_vocab_unique]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d03ccb6-c157-4908-8bb1-922bffc54175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efcb9e59-b379-431b-86c4-f1a6e02c5731",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loaded 55287 lines of words!\n",
      "Old data size: 55287\n",
      "New data size: 46743\n",
      "Removing15.45%\n",
      "avg_loss: 0.09509479388520288, total_loss: 190.18958777040575\n",
      "avg_loss: 189.4950125567164, total_loss: 379180.21470120316\n",
      "avg_loss: 175.78579240942202, total_loss: 730751.7995200459\n",
      "avg_loss: 164.82686127708274, total_loss: 1060405.5220742123\n",
      "avg_loss: 154.8455858189068, total_loss: 1370096.6937120275\n",
      "avg_loss: 146.45743218779853, total_loss: 1663011.5580876258\n",
      "avg_loss: 139.32458014934943, total_loss: 1941660.718386326\n",
      "avg_loss: 133.5475392099728, total_loss: 2208755.796806275\n",
      "avg_loss: 127.51892286598394, total_loss: 2463793.6425382574\n",
      "avg_loss: 123.31115868492317, total_loss: 2710415.959908099\n",
      "avg_loss: 119.68191087008374, total_loss: 2949779.781648273\n",
      "avg_loss: 115.99737608670561, total_loss: 3181774.5338216843\n",
      "avg_loss: 113.42923075021359, total_loss: 3408632.995322113\n",
      "avg_loss: 110.91079541478517, total_loss: 3630454.586151679\n",
      "avg_loss: 107.92298852057212, total_loss: 3846300.563192824\n",
      "avg_loss: 105.56846101388392, total_loss: 4057437.4852205953\n",
      "avg_loss: 104.26661699731848, total_loss: 4265970.719215243\n",
      "avg_loss: 102.2387496394, total_loss: 4470448.218494059\n",
      "avg_loss: 100.71752671332213, total_loss: 4671883.271920713\n",
      "avg_loss: 99.58397359619259, total_loss: 4871051.219113071\n",
      "avg_loss: 98.56992165778115, total_loss: 5068191.062428621\n",
      "avg_loss: 97.52590780138186, total_loss: 5263242.878031379\n",
      "avg_loss: 96.51656511995037, total_loss: 5456276.008271279\n",
      "avg_loss: 95.07931234400584, total_loss: 5646434.632959285\n",
      "Epoch 1/4, Loss: 5716750.661285518\n",
      "avg_loss: 0.05644072722618844, total_loss: 112.88145445237687\n",
      "avg_loss: 94.27740387242162, total_loss: 188667.6891992956\n",
      "avg_loss: 93.85473307219758, total_loss: 376377.1553436905\n",
      "avg_loss: 92.95467208064535, total_loss: 562286.4995049798\n",
      "avg_loss: 92.21901952160724, total_loss: 746724.5385481977\n",
      "avg_loss: 91.8794947247475, total_loss: 930483.5279976921\n",
      "avg_loss: 91.12481083017, total_loss: 1112733.1496580315\n",
      "avg_loss: 90.42961609465566, total_loss: 1293592.3818473378\n",
      "avg_loss: 90.07280348516923, total_loss: 1473737.988817667\n",
      "avg_loss: 89.91269222136427, total_loss: 1653563.3732603977\n",
      "avg_loss: 89.50956644926626, total_loss: 1832582.506158933\n",
      "avg_loss: 89.33016585481235, total_loss: 2011242.8378685594\n",
      "avg_loss: 89.00023140237023, total_loss: 2189243.3006733013\n",
      "avg_loss: 88.79016250433789, total_loss: 2366823.6256819773\n",
      "avg_loss: 88.25773190824657, total_loss: 2543339.0894984636\n",
      "avg_loss: 87.9395481651252, total_loss: 2719218.1858287184\n",
      "avg_loss: 87.97895050537129, total_loss: 2895176.0868394603\n",
      "avg_loss: 87.66597614178586, total_loss: 3070508.039123028\n",
      "avg_loss: 87.28540686229965, total_loss: 3245078.852847629\n",
      "avg_loss: 87.20164386109978, total_loss: 3419482.1405698275\n",
      "avg_loss: 87.08719283589564, total_loss: 3593656.5262416294\n",
      "avg_loss: 86.96796732855775, total_loss: 3767592.4608987384\n",
      "avg_loss: 86.633900539606, total_loss: 3940860.261977954\n",
      "avg_loss: 86.45630993395676, total_loss: 4113772.8818458673\n",
      "Epoch 2/4, Loss: 4177846.815340496\n",
      "avg_loss: 0.042135974038702007, total_loss: 84.27194807740402\n",
      "avg_loss: 86.23832346712817, total_loss: 172560.91888233373\n",
      "avg_loss: 86.25763161533503, total_loss: 345076.1821130049\n",
      "avg_loss: 86.1402299147832, total_loss: 517356.641942571\n",
      "avg_loss: 85.96805647050799, total_loss: 689292.7548835883\n",
      "avg_loss: 85.86689801427286, total_loss: 861026.5509121332\n",
      "avg_loss: 85.76869177604381, total_loss: 1032563.9344642219\n",
      "avg_loss: 85.54444823086546, total_loss: 1203652.830925951\n",
      "avg_loss: 85.47186059033416, total_loss: 1374596.552106621\n",
      "avg_loss: 85.46067292883728, total_loss: 1545517.8979642962\n",
      "avg_loss: 85.37508875881822, total_loss: 1716268.075481932\n",
      "avg_loss: 85.39366010015677, total_loss: 1887055.3956822471\n",
      "avg_loss: 85.3255657453462, total_loss: 2057706.5271729364\n",
      "avg_loss: 85.26833561075529, total_loss: 2228243.198394445\n",
      "avg_loss: 85.1354252479994, total_loss: 2398514.0488904435\n",
      "avg_loss: 85.02993259665186, total_loss: 2568573.914083737\n",
      "avg_loss: 85.02264734772419, total_loss: 2738619.2087791846\n",
      "avg_loss: 84.94724180455322, total_loss: 2908513.6923882873\n",
      "avg_loss: 84.96354062052983, total_loss: 3078440.7736293483\n",
      "avg_loss: 84.91260767510596, total_loss: 3248265.988979562\n",
      "avg_loss: 84.82004708197303, total_loss: 3417906.0831435085\n",
      "avg_loss: 84.85608616029268, total_loss: 3587618.255464086\n",
      "avg_loss: 84.67831347620555, total_loss: 3756974.882416491\n",
      "avg_loss: 84.6566184599628, total_loss: 3926288.1193364197\n",
      "Epoch 3/4, Loss: 3989106.356453999\n",
      "avg_loss: 0.04496465560370227, total_loss: 89.92931120740454\n",
      "avg_loss: 84.57737073235334, total_loss: 169244.6707759141\n",
      "avg_loss: 84.61007881959232, total_loss: 338464.82841509796\n",
      "avg_loss: 84.5921782883381, total_loss: 507649.18499177374\n",
      "avg_loss: 84.50663566489604, total_loss: 676662.4563215666\n",
      "avg_loss: 84.4973767039202, total_loss: 845657.2097294072\n",
      "avg_loss: 84.48221058971185, total_loss: 1014621.6309088321\n",
      "avg_loss: 84.46342350404288, total_loss: 1183548.4779169182\n",
      "avg_loss: 84.41564791446517, total_loss: 1352379.773745842\n",
      "avg_loss: 84.38750604474134, total_loss: 1521154.7858353287\n",
      "avg_loss: 84.38959247209189, total_loss: 1689933.9707795067\n",
      "avg_loss: 84.36200099661052, total_loss: 1858657.9727727259\n",
      "avg_loss: 84.36290116468237, total_loss: 2027383.7751020903\n",
      "avg_loss: 84.33031329315153, total_loss: 2196044.4016883895\n",
      "avg_loss: 84.27747881455473, total_loss: 2364599.3593175043\n",
      "avg_loss: 84.30156320598215, total_loss: 2533202.485729483\n",
      "avg_loss: 84.28054941056583, total_loss: 2701763.584550618\n",
      "avg_loss: 84.25772739815162, total_loss: 2870279.039346923\n",
      "avg_loss: 84.24489581747153, total_loss: 3038768.8309818613\n",
      "avg_loss: 84.2277095390434, total_loss: 3207224.250059956\n",
      "avg_loss: 84.19855023294508, total_loss: 3375621.3505258444\n",
      "avg_loss: 84.21706563277874, total_loss: 3544055.481791405\n",
      "avg_loss: 84.17351455988258, total_loss: 3712402.510911169\n",
      "avg_loss: 84.18147044074303, total_loss: 3880765.4517926597\n",
      "Epoch 4/4, Loss: 3943184.168471517\n"
     ]
    }
   ],
   "source": [
    "w2v = SlowoDwaWektor()\n",
    "w2v.load_data('data/task1_objects_contexts_polish.txt')\n",
    "w2v.filter_data(3)\n",
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c0b571a-b3af-494e-bbd5-c068659c4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.save_to_file('task1_w2v_vectors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4280ae27-8151-4895-8679-3e742c9a1010",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "suma = 0\n",
    "dupa=0\n",
    "visited = set()\n",
    "for w in w2v.cnt:\n",
    "    if w2v.cnt[w] > 3500:\n",
    "        assert w not in visited\n",
    "        visited.add(w)\n",
    "        suma += w2v.cnt[w]\n",
    "        dupa += 1\n",
    "print(dupa,'\\n',suma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a7e0173c-4736-4884-9ffe-e8e24ff11e21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "occ = list(w2v.cnt.values())\n",
    "fig, axs = plt.subplots(1, 1,\n",
    "                        figsize =(10, 7),\n",
    "                        tight_layout = True)\n",
    "occ = np.array(occ)\n",
    "occ = occ[occ < 1000]\n",
    "axs.hist(occ, bins = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ae909d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pies\n",
      "    mejoza 0.35160526633262634\n",
      "    przylot 0.35017895698547363\n",
      "    odwijać 0.3482518196105957\n",
      "    spazm 0.34482863545417786\n",
      "    zbliżać 0.3392108976840973\n",
      "    zastępczy 0.31202584505081177\n",
      "    żer 0.30797553062438965\n",
      "    zawrzeć 0.3032503128051758\n",
      "    złagodzenie 0.2995203733444214\n",
      "    samobadanie 0.29350408911705017\n",
      "\n",
      "WORD: smok\n",
      "    laleczka 0.3946513533592224\n",
      "    audyt 0.33622029423713684\n",
      "    przyklejać 0.3262861967086792\n",
      "    kręty 0.32525166869163513\n",
      "    spartakiada 0.3160785138607025\n",
      "    konstancja 0.3153826594352722\n",
      "    presupozycja 0.3103264272212982\n",
      "    teledysk 0.3102502226829529\n",
      "    richard 0.3047626316547394\n",
      "    trudniejszy 0.30339276790618896\n",
      "\n",
      "WORD: miłość\n",
      "    odrętwienie 0.40408554673194885\n",
      "    marka 0.35120901465415955\n",
      "    łagier 0.34148383140563965\n",
      "    bujny 0.33595356345176697\n",
      "    obcować 0.33426111936569214\n",
      "    wyświetlenie 0.33228573203086853\n",
      "    głup 0.3193388283252716\n",
      "    zwalnianie 0.30999845266342163\n",
      "    klara 0.3097013831138611\n",
      "    transza 0.30879828333854675\n",
      "\n",
      "WORD: rower\n",
      "    esej 0.31982889771461487\n",
      "    lekkoatletyka 0.31835541129112244\n",
      "    jagienka 0.31528303027153015\n",
      "    legionista 0.31337231397628784\n",
      "    oburzyć 0.31190618872642517\n",
      "    przetrwać 0.311474084854126\n",
      "    kobus 0.3038633465766907\n",
      "    konsolidowanie 0.29983457922935486\n",
      "    zagłuszyć 0.2994465231895447\n",
      "    przywrócić 0.2980682849884033\n",
      "\n",
      "WORD: maraton\n",
      "    dyktować 0.3512391448020935\n",
      "    cięcie 0.33050110936164856\n",
      "    witamina 0.3295639753341675\n",
      "    europa 0.32653170824050903\n",
      "    rysunek 0.3230399489402771\n",
      "    wjechać 0.3150961697101593\n",
      "    feldman 0.3099920451641083\n",
      "    wpadać 0.3098331093788147\n",
      "    zamieranie 0.309028685092926\n",
      "    amerykański 0.3052014410495758\n",
      "\n",
      "WORD: logika\n",
      "    kolonizacja 0.3629501461982727\n",
      "    pojedynek 0.3377896547317505\n",
      "    totalizator 0.3300153911113739\n",
      "    nawrocki 0.3249160349369049\n",
      "    podsuwać 0.3238339424133301\n",
      "    insulina 0.3114911913871765\n",
      "    wytop 0.3092297613620758\n",
      "    burzyński 0.30889493227005005\n",
      "    świątkowski 0.3085102438926697\n",
      "    wzmocnić 0.30728620290756226\n",
      "\n",
      "WORD: motyl\n",
      "    zaprogramowany 0.3744083046913147\n",
      "    politykierstwo 0.36174559593200684\n",
      "    przedłożyć 0.36154207587242126\n",
      "    zaciekawienie 0.3384195566177368\n",
      "    wektor 0.33773767948150635\n",
      "    aureola 0.33442574739456177\n",
      "    cyranka 0.33270466327667236\n",
      "    piłeczka 0.3168846666812897\n",
      "    andora 0.31401216983795166\n",
      "    zestaw 0.31192314624786377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors.txt', binary=False)\n",
    "\n",
    "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
    "example_polish_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
    "\n",
    "example_words = example_polish_words\n",
    "\n",
    "for w0 in example_words:\n",
    "    print ('WORD:', w0)\n",
    "    for w, v in task1_wv.most_similar(w0):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c75ce01f-288f-4eee-bf15-680b8c4f20d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sahara 0.33845117688179016\n",
      "    konstytucjonalizm 0.3321194350719452\n",
      "    mysz 0.327793687582016\n",
      "    przepakować 0.32736051082611084\n",
      "    rozwaga 0.3054957687854767\n",
      "    świerczewski 0.3023218512535095\n",
      "    wyeliminowanie 0.2995588183403015\n",
      "    zwarty 0.2988737225532532\n",
      "    jabłonna 0.29872727394104004\n",
      "    awans 0.29787787795066833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task1_wv.similarity('kobieta', 'mężczyzna')\n",
    "for w, v in task1_wv.most_similar('mężczyzna'):\n",
    "    print ('   ', w, v)\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41228961",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2 (4 points)\n",
    "\n",
    "Your task is to train the embeddings for Simple Wikipedia titles, using gensim library. As the example below shows, training is really simple:\n",
    "\n",
    "```python\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "```\n",
    "*sentences* can be a list of list of tokens, you can also use *gensim.models.word2vec.LineSentence(source)* to create restartable iterator from file. At first, use [this file] containing such pairs of titles, that one article links to another.\n",
    "\n",
    "We say that two titles are *related* if they both contain a word (or a word bigram) which is not very popular (it occurs only in several titles). Make this definition more precise, and create the corpora which contains pairs of related titles. Make a mixture of the original corpora, and the new one, then train title vectors again.\n",
    "\n",
    "Compare these two approaches using similar code to the code from Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2b83f-d1b9-4c70-8408-951c4af48495",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ex. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f6ad8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiLink:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, path = \"data/task2_simple.wiki.links.txt.gz\"):\n",
    "        self.path = path\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with gzip.open(self.path, 'rt') as f:\n",
    "            for line in f:\n",
    "                yield line.strip().split()\n",
    "                \n",
    "wiki_corpora = WikiLink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc130c23-3c04-4f9c-9372-e9cd70a4cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=wiki_corpora, \n",
    "    vector_size=100, \n",
    "    window=5, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")\n",
    "model.save(\"word2vec_wiki.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34e944ac-a12c-4d9b-bba7-16c66d3980c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"word2vec_wiki.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34e5fec3-0256-42b1-9468-149ecbda75b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7571390it [00:15, 497649.65it/s]\n"
     ]
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "wiki_corpora_size = 0\n",
    "\n",
    "for title in tqdm(wiki_corpora):\n",
    "    wiki_corpora_size += 1\n",
    "    for word in title:\n",
    "        word_freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbffa9e6-4574-4a11-9a53-30d8b2fc1c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 7571390/7571390 [00:36<00:00, 209305.24it/s]\n"
     ]
    }
   ],
   "source": [
    "threshold = 500\n",
    "related_wiki_corpora = []\n",
    "\n",
    "for title_pair in tqdm(wiki_corpora, total=wiki_corpora_size):\n",
    "    if len(title_pair) == 2:\n",
    "        t1 = re.split(r'_|:|\\\\|\\/|-', title_pair[0])\n",
    "        t2 = re.split(r'_|:|\\\\|\\/|-', title_pair[1])\n",
    "        t1 = [w for w in t1 if len(w) > 0]\n",
    "        t2 = [w for w in t2 if len(w) > 0]\n",
    "        if any(word_freq[word] < threshold for word in set(t1).intersection(t2)):\n",
    "            related_wiki_corpora.append(t1 + t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79ff41d5-78d7-4123-af3a-aea39baa3f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11820286631648878"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(related_wiki_corpora) / wiki_corpora_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d76a8e0-e85e-47bb-aa58-35d5e30420b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=related_wiki_corpora, \n",
    "    vector_size=100, \n",
    "    window=5, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")\n",
    "model.save(\"word2vec_wiki_related.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6082f0ae-df4e-4686-aea0-212fad8b2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_related = gensim.models.Word2Vec.load(\"word2vec_wiki_related.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e3fd26f-cb7a-43d3-94d9-c0c9faee143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG model: [france]\n",
      "    communes_of_france 0.929648756980896\n",
      "    departments_of_france 0.926301896572113\n",
      "    regions_of_france 0.9084035754203796\n",
      "    paris 0.8786289691925049\n",
      "    pays_de_la_loire 0.850056529045105\n",
      "    commune_in_france 0.8354650735855103\n",
      "    brittany 0.8287339210510254\n",
      "    nord-pas-de-calais 0.8194249272346497\n",
      "    template:pas-de-calais_communes 0.8097964525222778\n",
      "    somme 0.8059446811676025\n",
      "RELATED model[france]:\n",
      "    french 0.7099596261978149\n",
      "    belgium 0.6925283074378967\n",
      "    italy 0.6833450794219971\n",
      "    sweden 0.6565394997596741\n",
      "    england 0.6536797285079956\n",
      "    europe 0.6512018442153931\n",
      "    romania 0.627118706703186\n",
      "    paris 0.6097730398178101\n",
      "    switzerland 0.6019037961959839\n",
      "    spain 0.6005282402038574\n",
      "\n",
      "OG model: [canada]\n",
      "    british_columbia 0.8625205755233765\n",
      "    toronto 0.8618823885917664\n",
      "    ontario 0.8454139232635498\n",
      "    quebec 0.8440713286399841\n",
      "    vancouver 0.8372489213943481\n",
      "    usa 0.8226689696311951\n",
      "    coast_mountains 0.8152483701705933\n",
      "    montreal 0.8083784580230713\n",
      "    alberta 0.802631676197052\n",
      "    new_zealand 0.79205721616745\n",
      "RELATED model[canada]:\n",
      "    canadian 0.7736265659332275\n",
      "    australia 0.6662326455116272\n",
      "    quebec 0.6559069752693176\n",
      "    alberta 0.6558446288108826\n",
      "    manitoba 0.6449616551399231\n",
      "    ontario 0.6263796091079712\n",
      "    british 0.621772825717926\n",
      "    iran 0.5882902145385742\n",
      "    saskatchewan 0.573298990726471\n",
      "    montreal 0.5707108974456787\n",
      "\n",
      "OG model: [actor]\n",
      "    singer 0.9829621315002441\n",
      "    actress 0.9826483130455017\n",
      "    television 0.9768034219741821\n",
      "    movie 0.9593040943145752\n",
      "    comedian 0.9527757167816162\n",
      "    movie_director 0.9485150575637817\n",
      "    musician 0.9483770728111267\n",
      "    category:actors_from_new_york_city 0.9462528228759766\n",
      "    category:american_movie_actors 0.9443041682243347\n",
      "    category:american_television_actors 0.9412195682525635\n",
      "RELATED model[actor]:\n",
      "    supporting 0.8988872766494751\n",
      "    actress 0.8418369889259338\n",
      "    feature 0.7407505512237549\n",
      "    best 0.7350122332572937\n",
      "    bafta 0.7063173055648804\n",
      "    avn 0.6955521106719971\n",
      "    primetime 0.6808863878250122\n",
      "    film 0.6801540851593018\n",
      "    award 0.6763088703155518\n",
      "    emmy 0.6744867563247681\n",
      "\n",
      "OG model: [war]\n",
      "    flood 0.9875231385231018\n",
      "    caribbean 0.9870839715003967\n",
      "    winter 0.9870537519454956\n",
      "    military 0.9865217208862305\n",
      "    summer 0.9855738878250122\n",
      "    mining 0.9853763580322266\n",
      "    execution 0.9852871298789978\n",
      "    police 0.9838403463363647\n",
      "    americas 0.9837527871131897\n",
      "    education 0.9835606217384338\n",
      "RELATED model[war]:\n",
      "    military 0.6324674487113953\n",
      "    servants 0.586714506149292\n",
      "    war.gif 0.5793567299842834\n",
      "    soldiers 0.5776336789131165\n",
      "    veterans 0.5745684504508972\n",
      "    war, 0.5732569694519043\n",
      "    due.jpg 0.556089460849762\n",
      "    war.jpg 0.5535596609115601\n",
      "    during 0.5374757647514343\n",
      "    war}} 0.536090075969696\n",
      "\n",
      "OG model: [animal]\n",
      "    mammal 0.9706447720527649\n",
      "    species 0.969950795173645\n",
      "    plant 0.9680063724517822\n",
      "    bird 0.9510186910629272\n",
      "    genus 0.9506730437278748\n",
      "    insect 0.9361304640769958\n",
      "    bacteria 0.931740403175354\n",
      "    wikipedia:list_of_articles_all_languages_should_have/expanded/biology_and_health_sciences 0.9121317863464355\n",
      "    family_(biology) 0.9105764627456665\n",
      "    fish 0.9005780220031738\n",
      "RELATED model[animal]:\n",
      "    crossing 0.8326018452644348\n",
      "    phyla 0.7984423637390137\n",
      "    enclosures 0.7952112555503845\n",
      "    hybrids 0.6722685098648071\n",
      "    tissues 0.6658265590667725\n",
      "    billups 0.645854651927948\n",
      "    electronics 0.6323155164718628\n",
      "    scientific 0.6181077361106873\n",
      "    marvel 0.6153817176818848\n",
      "    terrestrial 0.6146643757820129\n",
      "\n",
      "OG model: [lion]\n",
      "    tiger 0.9840335249900818\n",
      "    alphabet 0.9814127087593079\n",
      "    monkey 0.9794914722442627\n",
      "    rice 0.978926956653595\n",
      "    chicken 0.9784526228904724\n",
      "    sugar 0.9776687622070312\n",
      "    crop 0.9738220572471619\n",
      "    bronze_age 0.972710907459259\n",
      "    bamboo 0.972592830657959\n",
      "    pig 0.9720938205718994\n",
      "RELATED model[lion]:\n",
      "    simba's 0.7972932457923889\n",
      "    king) 0.7880637049674988\n",
      "    scar 0.7556650638580322\n",
      "    tut's 0.7368935942649841\n",
      "    godzilla 0.7116765379905701\n",
      "    scorpion 0.7099709510803223\n",
      "    zira 0.6730484366416931\n",
      "    mummy 0.6705577373504639\n",
      "    cepheus, 0.6659989356994629\n",
      "    curse 0.6641641855239868\n",
      "\n",
      "OG model: [angle]\n",
      "    shape 0.9971457123756409\n",
      "    iron_oxide 0.9968875050544739\n",
      "    atomic_orbital 0.996859073638916\n",
      "    pnictogen 0.9967188835144043\n",
      "    sweat 0.9965899586677551\n",
      "    probability 0.9965434074401855\n",
      "    water_(molecule) 0.9965274930000305\n",
      "    formula 0.9965195655822754\n",
      "    boson 0.9964831471443176\n",
      "    bacterium 0.9963256120681763\n",
      "RELATED model[angle]:\n",
      "    obtuse 0.9104016423225403\n",
      "    triangle 0.8471346497535706\n",
      "    isosceles 0.8324440717697144\n",
      "    format) 0.81600421667099\n",
      "    supplementary 0.8085951805114746\n",
      "    sierpinski 0.8049046397209167\n",
      "    sperm 0.8031500577926636\n",
      "    equilateral 0.8001793026924133\n",
      "    tension 0.7957285046577454\n",
      "    (geometry) 0.7935285568237305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_words = [\"france\", \"canada\", \"actor\", \n",
    "                 \"war\", \"animal\", \"lion\", \"angle\"]\n",
    "\n",
    "for w0 in example_words:\n",
    "    print (f'OG model: [{w0}]')\n",
    "    for w, v in model.wv.most_similar(w0):\n",
    "        print ('   ', w, v)\n",
    "    print(f\"RELATED model[{w0}]:\")   \n",
    "    for w, v in model_related.wv.most_similar(w0):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711fdca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task 3 (4 points)\n",
    "\n",
    "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
    "\n",
    "<pre>\n",
    "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
    "</pre>\n",
    "\n",
    "And this is its translation into Lower:\n",
    "\n",
    "<pre>\n",
    "the quick brown fox jumps over the lazy dog\n",
    "</pre>\n",
    "\n",
    "You have two corpora for these languages (with different sentences). Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
    "\n",
    "Prepare the corpora wich contains three kind of sentences:\n",
    "* Upper corpus sentences\n",
    "* Lower corpus sentences\n",
    "* sentences derived from Upper/Lower corpus, modified using D\n",
    "\n",
    "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
    "\n",
    "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
    "\n",
    "<pre>\n",
    "WOLF, CAT, WOLVES, LION, gopher, dog\n",
    "</pre>\n",
    "\n",
    "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1d818-b994-4164-b840-eb1833eff509",
   "metadata": {},
   "source": [
    "## Ex. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d5064f5-c7af-4caf-bc28-9927ed38b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "13de8d3e-3eb0-4c11-8130-13eefbda7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpora:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with gzip.open(self.path, 'rt') as f:\n",
    "            for line in f:\n",
    "                yield line.strip().split()\n",
    "            \n",
    "lower_corpus = list(Corpora(\"data/task3_polish_lower.txt.gz\"))\n",
    "upper_corpus = list(Corpora(\"data/task3_polish_upper.txt.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cea4180f-fc5d-4a7c-9314-9e779c432fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab_up = set()\n",
    "full_vocab_low = set()\n",
    "\n",
    "for s in lower_corpus:\n",
    "    for w in s:\n",
    "        if len(w) > 2:\n",
    "            full_vocab_low.add(w.lower())\n",
    "\n",
    "for s in upper_corpus:\n",
    "    for w in s:\n",
    "        if len(w) > 2:\n",
    "            full_vocab_up.add(w.lower())\n",
    "            \n",
    "common_vocab = full_vocab_up.intersection(full_vocab_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "573052a1-9691-4648-a521-1447f032391a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97534/1819341793.py:1: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  random_words = random.sample(common_vocab, k=1000)\n"
     ]
    }
   ],
   "source": [
    "random_words = random.sample(common_vocab, k=1000)\n",
    "\n",
    "low2up = {w.lower(): w.upper() for w in random_words}\n",
    "up2low = {w.upper(): w.lower() for w in random_words}\n",
    "\n",
    "out_words = [w for w in common_vocab if w not in random_words]\n",
    "random_out_words = random.sample(out_words, k=10000)\n",
    "\n",
    "# Modify Upper/Lower corpus using D\n",
    "def modify_corpus(corpus, D):\n",
    "    return [[D[word] if word in D else word for word in sentence] for sentence in corpus]\n",
    "\n",
    "upper_corpus_modified = modify_corpus(upper_corpus, up2low)\n",
    "lower_corpus_modified = modify_corpus(lower_corpus, low2up)\n",
    "\n",
    "# Combine corpora for joint training\n",
    "combined_corpus = upper_corpus + lower_corpus + upper_corpus_modified + lower_corpus_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "68088e85-6168-47e5-aa22-435f0b53b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=combined_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save('word2vec_uplow.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "69993543-0241-4cec-8d8a-c94b7e820bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"word2vec_wiki.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "65a99120-41a1-4134-8037-a20f18304371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for words in D: 0.9964001384562133\n",
      "Average score for words out of D: 0.2921319102353585\n"
     ]
    }
   ],
   "source": [
    "# Compute scores for each word\n",
    "def compute_score(word):\n",
    "    similar_words = model.wv.most_similar(word)\n",
    "    similar_words = [w for w, _ in similar_words]\n",
    "    if word.lower() in similar_words:\n",
    "        return 1 / (similar_words.index(word.lower()) + 1)\n",
    "    else:\n",
    "        return 0\n",
    "random_out_words = random.sample(out_words, k=10000)\n",
    "\n",
    "# Compute average scores separately for words from D and for words out of D\n",
    "in_d_scores = [compute_score(word.upper()) for word in random_words]\n",
    "out_of_d_scores = [compute_score(word.upper()) for word in random_out_words]\n",
    "\n",
    "non_zero_in = 0\n",
    "for score in in_d_scores:\n",
    "    if score:\n",
    "        non_zero_in+=1\n",
    "non_zero_out = 0\n",
    "for score in out_of_d_scores:\n",
    "    if score:\n",
    "        non_zero_out+=1\n",
    "\n",
    "print(f\"Average score for words in D: {sum(in_d_scores) / non_zero_in}\")\n",
    "print(f\"Average score for words out of D: {sum(out_of_d_scores) / non_zero_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947e307",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task 4 (4 points)\n",
    "\n",
    "In this task you are asked to do two things:\n",
    "1. compare the embeddings computed on small corpus (like Brown Corpus , see: <https://en.wikipedia.org/wiki/Brown_Corpus>) with the ones coming from Google News Corpus\n",
    "2. Try to use other resourses like WordNet to enrich to corpus, and obtain better embeddings\n",
    "\n",
    "You can use the following code snippets:\n",
    "\n",
    "```python\n",
    "# printing tokenized Brown Corpora\n",
    "from nltk.corpus import brown\n",
    "for s in brown.sents():\n",
    "    print(*s)\n",
    "    \n",
    "#iterating over all synsets in WordNet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for synset_type in 'avrns': # n == noun, v == verb, ...\n",
    "    for synset in list(wn.all_synsets(synset_type)))[:10]:\n",
    "        print (synset.definition())\n",
    "        print (synset.examples())\n",
    "        print ([lem.name() for lem in synset.lemmas()])\n",
    "        print (synset.hyperonims()) # nodes 1 level up in ontology\n",
    "        \n",
    "# loading model and compute cosine similarity between words\n",
    "\n",
    "model = Word2Vec.load('models/w2v.wordnet5.model') \n",
    "print (model.wv.similarity('dog', 'cat'))\n",
    "```\n",
    "\n",
    "Embeddings will be tested using WordSim-353 dataset, the code showing the quality is in the cell below. Prepare the following corpora:\n",
    "1. Tokenized Brown Corpora\n",
    "2. Definitions and examples from Princeton WordNet\n",
    "3. (1) and (2) together\n",
    "4. (3) enriched with pseudosentences containing (a subset) of WordNet knowledge (such as 'tiger is a carnivore')\n",
    "\n",
    "Train 4 Word2Vec models, and raport Spearman correletion between similarities based on your vectors, and similarities based on human judgements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfd265-8db5-48c1-8a32-8d12e0a9c21c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ex. 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e721413-7df6-4ccf-8254-8bfb2ef606ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/maurycy/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/maurycy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2f275a-27d0-4496-9a68-4d8915d829fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus = list(brown.sents())\n",
    "\n",
    "wordnet_corpus = []\n",
    "for synset_type in 'avrns':\n",
    "    for synset in list(wn.all_synsets(synset_type)):\n",
    "        definition = synset.definition().split()\n",
    "        wordnet_corpus.append(definition)\n",
    "        for example in synset.examples():\n",
    "            wordnet_corpus.append(example.split())\n",
    "\n",
    "combined_corpus = brown_corpus + wordnet_corpus\n",
    "\n",
    "enriched_corpus = combined_corpus.copy()\n",
    "for synset in list(wn.all_synsets()):\n",
    "    pseudosentence = synset.hypernym_paths()[0]\n",
    "    pseudosentence = [lemma.name() for syn in pseudosentence for lemma in syn.lemmas()]\n",
    "    enriched_corpus.append(pseudosentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80399d05-631a-4715-baf6-a88d22ee888e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(corpus)=57340\n",
      "len(corpus)=189288\n",
      "len(corpus)=246628\n",
      "len(corpus)=364287\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for corpus in [brown_corpus, wordnet_corpus, combined_corpus, enriched_corpus]:\n",
    "    print(f'len(corpus)={len(corpus)}')\n",
    "    model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e8fdc22-300f-446b-926c-ef10c973f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "banned = set()\n",
    "\n",
    "with open(datapath(\"wordsim353.tsv\")) as file:\n",
    "    for line in file:\n",
    "        if line[0] != '#':\n",
    "            word1, word2, score = line.strip().split('\\t')\n",
    "            for model in models:\n",
    "                try:\n",
    "                    model.wv.similarity(word1, word2)\n",
    "                except KeyError:\n",
    "                    banned.add((word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f98f6b3f-bd27-465b-a014-0809373a8102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation and pvals:\n",
      "Brown Corpus:          -0.003833532598177231, pval: 0.9459701464722843\n",
      "WordNet Corpus:    0.23826020839468162, pval: 1.9234973267081473e-05\n",
      "Combined Corpus:       0.3172703912315073, pval: 8.509300936622935e-09\n",
      "Enriched Corpus:       0.3960375408291786, pval: 2.8406363593103117e-13\n"
     ]
    }
   ],
   "source": [
    "# Load WordSim-353 dataset and calculate similarities\n",
    "\n",
    "similarity = [[], [], [], []]\n",
    "human = []\n",
    "\n",
    "with open(datapath(\"wordsim353.tsv\")) as file:\n",
    "    for line in file:\n",
    "        if line[0] != '#':\n",
    "            word1, word2, score = line.strip().split('\\t')\n",
    "            if (word1, word2) not in banned:\n",
    "                score = float(score)\n",
    "                human.append(score)\n",
    "                similarities = []\n",
    "                for i, model in enumerate(models):\n",
    "                        similarity[i].append(model.wv.similarity(word1, word2))\n",
    "\n",
    "# Calculate Spearman correlation between model similarities and human judgments\n",
    "correlations, pvals = [], []\n",
    "for i in range(4):\n",
    "    correlation, pval = spearmanr(similarity[i], human)\n",
    "    correlations.append(correlation)\n",
    "    pvals.append(pval)\n",
    "\n",
    "print(\"Spearman Correlation and pvals:\")\n",
    "print(f\"Brown Corpus:          {correlations[0]}, pval: {pvals[0]}\")\n",
    "print(f\"WordNet Corpus:    {correlations[1]}, pval: {pvals[1]}\")\n",
    "print(f\"Combined Corpus:       {correlations[2]}, pval: {pvals[2]}\")\n",
    "print(f\"Enriched Corpus:       {correlations[3]}, pval: {pvals[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802bd33d",
   "metadata": {},
   "source": [
    "# Task 5 (4 points)\n",
    "\n",
    "Do the Problem 1 from old version of [Assigment 4](https://github.com/rnoxy/dl_uwr/blob/summer2023/Assignments/Assignment4.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49d8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
